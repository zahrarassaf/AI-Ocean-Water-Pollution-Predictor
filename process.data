import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import urllib.request
import zipfile

print("=" * 60)
print("OCEAN DATA PROCESSOR")
print("=" * 60)

def check_files():
    """Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯"""
    
    raw_path = "data/raw/"
    processed_path = "data/processed/"
    
    os.makedirs(processed_path, exist_ok=True)
    
    files = os.listdir(raw_path)
    print(f"Files in data/raw/: {files}")
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§ÛŒØ² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
    for file in files:
        file_path = os.path.join(raw_path, file)
        size_kb = os.path.getsize(file_path) / 1024
        print(f"  {file}: {size_kb:.1f} KB")
    
    return files

def create_sample_ocean_data():
    """Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ù‚ÛŒØ§Ù†ÙˆØ³ÛŒ"""
    
    print("\nCreating sample ocean water quality data...")
    
    np.random.seed(42)
    n_samples = 2000
    
    # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø§ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ
    data = {
        # Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ ÙÛŒØ²ÛŒÚ©ÛŒ
        'sea_surface_temp': np.random.uniform(10, 35, n_samples),  # Ø¯Ù…Ø§ÛŒ Ø³Ø·Ø­ Ø¯Ø±ÛŒØ§ (Â°C)
        'salinity': np.random.uniform(30, 38, n_samples),  # Ø´ÙˆØ±ÛŒ (PSU)
        'turbidity': np.random.uniform(0.1, 15, n_samples),  # Ú©Ø¯ÙˆØ±Øª (NTU)
        
        # Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø´ÛŒÙ…ÛŒØ§ÛŒÛŒ
        'ph': np.random.uniform(7.5, 8.4, n_samples),  # Ø§Ø³ÛŒØ¯ÛŒØªÙ‡
        'dissolved_oxygen': np.random.uniform(4, 12, n_samples),  # Ø§Ú©Ø³ÛŒÚ˜Ù† Ù…Ø­Ù„ÙˆÙ„ (mg/L)
        'nitrate': np.random.uniform(0, 8, n_samples),  # Ù†ÛŒØªØ±Ø§Øª (mg/L)
        'phosphate': np.random.uniform(0, 1.5, n_samples),  # ÙØ³ÙØ§Øª (mg/L)
        'ammonia': np.random.uniform(0, 0.5, n_samples),  # Ø¢Ù…ÙˆÙ†ÛŒØ§Ú© (mg/L)
        
        # Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø¨ÛŒÙˆÙ„ÙˆÚ˜ÛŒÚ©ÛŒ
        'chlorophyll_a': np.random.uniform(0.01, 10, n_samples),  # Ú©Ù„Ø±ÙˆÙÛŒÙ„-a (mg/mÂ³)
        'sechi_depth': np.random.uniform(1, 30, n_samples),  # Ø¹Ù…Ù‚ Ø³Ú†ÛŒ (Ù…ØªØ±)
        
        # ÙÙ„Ø²Ø§Øª Ø³Ù†Ú¯ÛŒÙ†
        'lead': np.random.uniform(0, 0.05, n_samples),  # Ø³Ø±Ø¨ (mg/L)
        'mercury': np.random.uniform(0, 0.002, n_samples),  # Ø¬ÛŒÙˆÙ‡ (mg/L)
        'cadmium': np.random.uniform(0, 0.01, n_samples),  # Ú©Ø§Ø¯Ù…ÛŒÙˆÙ… (mg/L)
        
        # Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ
        'latitude': np.random.uniform(-90, 90, n_samples),
        'longitude': np.random.uniform(-180, 180, n_samples),
        
        # Ø²Ù…Ø§Ù†
        'month': np.random.randint(1, 13, n_samples),
    }
    
    df = pd.DataFrame(data)
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø³ØªÙˆÙ† target (Ø³Ø·Ø­ Ø¢Ù„ÙˆØ¯Ú¯ÛŒ) Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ±Ú©ÛŒØ¨ÛŒ Ø§Ø² Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
    pollution_score = (
        df['chlorophyll_a'] * 0.3 +  # Ø´Ú©ÙˆÙØ§ÛŒÛŒ Ø¬Ù„Ø¨Ú©ÛŒ
        df['nitrate'] * 0.2 +  # Ù…ÙˆØ§Ø¯ Ù…ØºØ°ÛŒ
        df['phosphate'] * 0.15 +
        df['lead'] * 100 +  # ÙÙ„Ø²Ø§Øª Ø³Ù†Ú¯ÛŒÙ† (Ø¶Ø±ÛŒØ¨ Ø¨Ø§Ù„Ø§)
        df['mercury'] * 500 +
        df['ammonia'] * 0.1
    )
    
    # Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ù‡ Û³ Ø³Ø·Ø­
    df['pollution_level'] = pd.qcut(pollution_score, q=3, labels=[0, 1, 2])
    
    # 0: Ú©Ù… (Low), 1: Ù…ØªÙˆØ³Ø· (Medium), 2: Ø¨Ø§Ù„Ø§ (High)
    
    return df

def process_for_ml(df):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†"""
    
    processed_path = "data/processed/"
    
    print(f"\nProcessing {len(df)} samples...")
    print(f"Original shape: {df.shape}")
    
    # Ø­Ø°Ù Ù…Ù‚Ø§Ø¯ÛŒØ± NaN
    df = df.dropna()
    print(f"After removing NaN: {df.shape}")
    
    # Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† features Ùˆ target
    X = df.drop('pollution_level', axis=1)
    y = df['pollution_level']
    
    # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # Ø°Ø®ÛŒØ±Ù‡
    X_train.to_csv(f"{processed_path}/X_train.csv", index=False)
    X_test.to_csv(f"{processed_path}/X_test.csv", index=False)
    y_train.to_csv(f"{processed_path}/y_train.csv", index=False)
    y_test.to_csv(f"{processed_path}/y_test.csv", index=False)
    
    # Ù‡Ù…Ú†Ù†ÛŒÙ† ÛŒÚ© ÙØ§ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø°Ø®ÛŒØ±Ù‡ Ú©Ù†ÛŒÙ…
    df.to_csv(f"{processed_path}/full_ocean_data.csv", index=False)
    
    print(f"\nâœ… Data processing completed!")
    print(f"   Training set: {len(X_train)} samples")
    print(f"   Test set: {len(X_test)} samples")
    print(f"   Features: {X_train.shape[1]}")
    print(f"   Target classes: {sorted(y.unique())}")
    print(f"   Class distribution:")
    print(y.value_counts().sort_index())
    
    # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø±ÛŒ
    print("\nğŸ“Š Sample statistics:")
    print(df[['chlorophyll_a', 'nitrate', 'phosphate', 'lead', 'pollution_level']].describe())
    
    return X_train, X_test, y_train, y_test

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    
    print("Starting ocean data processing...\n")
    
    # Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
    files = check_files()
    
    # Ø§Ú¯Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ NetCDF Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±Ù†Ø¯ØŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    if files and any(f.endswith('.nc') for f in files):
        print("\nâš ï¸ NetCDF files detected but may be corrupted.")
        print("Using sample data for now...")
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡
    ocean_df = create_sample_ocean_data()
    
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø±Ø§ÛŒ ML
    process_for_ml(ocean_df)
    
    # Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© ÙØ§ÛŒÙ„ README Ø¨Ø±Ø§ÛŒ ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    create_readme()
    
    print("\n" + "=" * 60)
    print("READY FOR MODEL TRAINING!")
    print("=" * 60)
    print("\nNow run: python train_final.py")

def create_readme():
    """Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ ØªÙˆØ¶ÛŒØ­Ø§Øª"""
    
    readme_content = """# Ocean Water Quality Dataset

## Variables Description:

### Physical Parameters:
- sea_surface_temp: Sea surface temperature (Â°C)
- salinity: Salinity (PSU)
- turbidity: Water turbidity (NTU)

### Chemical Parameters:
- ph: Acidity level
- dissolved_oxygen: Dissolved oxygen (mg/L)
- nitrate: Nitrate concentration (mg/L)
- phosphate: Phosphate concentration (mg/L)
- ammonia: Ammonia concentration (mg/L)

### Biological Parameters:
- chlorophyll_a: Chlorophyll-a concentration (mg/mÂ³)
- sechi_depth: Secchi disk depth (m)

### Heavy Metals:
- lead: Lead concentration (mg/L)
- mercury: Mercury concentration (mg/L)
- cadmium: Cadmium concentration (mg/L)

### Geographical & Temporal:
- latitude: Latitude coordinate
- longitude: Longitude coordinate
- month: Month of observation (1-12)

### Target:
- pollution_level: Pollution level (0=Low, 1=Medium, 2=High)

## Data Source:
This is synthetic data created for AI model training.
For real data, replace with actual ocean monitoring data.

## Usage:
1. Train model: python train_final.py
2. Make predictions: python predict.py
"""
    
    with open("data/processed/DATA_DESCRIPTION.md", "w") as f:
        f.write(readme_content)

if __name__ == "__main__":
    main()

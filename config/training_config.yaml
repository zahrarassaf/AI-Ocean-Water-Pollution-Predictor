# Training Configuration

# Data settings
data:
  input_path: "data/processed"
  output_path: "data/processed"
  format: "netcdf"
  variables:
    - chl
    - kd490
    - zsd
    - current_speed
    - current_direction
    - temperature
    - salinity
  target_variable: pp
  time_range: ["2010-01-01", "2020-12-31"]
  spatial_range:
    lat: [30, 45]
    lon: [-130, -110]

# Preprocessing
preprocessing:
  missing_values:
    method: interpolate
    limit: 3
  outliers:
    method: iqr
    threshold: 3.0
    handling: clip
  scaling:
    method: standard
  filters:
    temporal:
      window_size: 7
      type: savgol

# Feature engineering
features:
  interactions: true
  polynomials: true
  temporal: true
  ratios: true
  derived_features: true

# Models
models:
  random_forest:
    n_models: 10
    n_estimators: 200
    max_depth: null
    min_samples_split: 2
    min_samples_leaf: 1
    max_features: sqrt
    bootstrap: true
  
  gaussian_process:
    n_models: 3
    kernels:
      - rbf
      - matern
      - rational_quadratic
    max_samples: 2000
  
  neural_network:
    hidden_layers: [128, 64, 32]
    dropout_rate: 0.3
    learning_rate: 0.001
    batch_size: 64
    epochs: 100
    early_stopping_patience: 10
  
  ensemble:
    method: weighted_average

# Training
training:
  test_size: 0.2
  validation_size: 0.1
  random_state: 42
  n_folds: 5
  early_stopping: true
  patience: 10
  max_epochs: 100

# Hardware
hardware:
  use_gpu: true
  n_workers: 4
  memory_limit: "4GB"

# Output
output:
  directory: "results"
  save_models: true
  save_predictions: true
  save_plots: true
  report_format: "html"
